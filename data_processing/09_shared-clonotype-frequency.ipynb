{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared clonotype frequency\n",
    "\n",
    "For every groupwise combination of two or more years, compute the frequency of universally shared clonotypes (that is, clonotypes found in the repertoire of every year in the group).\n",
    "\n",
    "The following Python packages are required to run the code in this notebook:\n",
    "  * numpy\n",
    "  * pandas\n",
    "  * [abutils](https://github.com/briney/abutils)\n",
    "\n",
    "They can be install by running `pip install numpy pandas abutils`\n",
    "\n",
    "*NOTE: this notebook requires the use of the Unix command line tool `wc`. Thus, it requires a Unix-based operating system to run correctly (MacOS and most flavors of Linux should be fine). Running this notebook on Windows 10 may be possible using the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about) but we have not tested this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### years, files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files and directories\n",
    "dedup_year_dir = './data/dedup_year_clonotype_pools/'\n",
    "cross_year_occurance_files = list_files('./data/user-calculated_cross-year_clonotype_duplicate-counts/')\n",
    "\n",
    "# years\n",
    "with open('./data/years.txt') as f:\n",
    "    years = sorted(f.read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique clonotypes per year\n",
    "\n",
    "If you'd like to actually count the number of unique clonotypes per year, you can run the code in [**this**](LINK) notebook or download a dataset containing each year's unique clonotypes [**here**](LINK). Note that the decompressed unique clonotype dataset is fairly large (about 8GB). \n",
    "\n",
    "All we're doing is counting the number of lines in the unique clonotype file. If you'd rather not download and decompress the data just to count the lines, skip the next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['327059-2016', '327059-2020', 'D103-2016', 'D103-2021']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327059-2016\n",
      "327059-2020\n",
      "D103-2016\n",
      "D103-2021\n"
     ]
    }
   ],
   "source": [
    "year_sizes = {}\n",
    "for year in years:\n",
    "    print(year)\n",
    "    dedup_file = os.path.join(dedup_year_dir, '{}_dedup_pool_vj-aa.txt'.format(year))\n",
    "    wc_cmd = 'wc -l {}'.format(dedup_file)\n",
    "    p = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = p.communicate()\n",
    "    size = int(stdout.strip().split()[0])\n",
    "    year_sizes[year] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'327059-2016': 9281663,\n",
       " '327059-2020': 3619333,\n",
       " 'D103-2016': 2917577,\n",
       " 'D103-2021': 1536004}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify shared clonotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15/15) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (00:00)  \n"
     ]
    }
   ],
   "source": [
    "shared_frequencies_by_group_size = {i + 1: [] for i in range(len(years))}\n",
    "shared_frequencies_by_group = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "progress_bar(0, len(cross_year_occurance_files), start_time=start_time)\n",
    "\n",
    "span=2\n",
    "\n",
    "for i, of in enumerate(cross_year_occurance_files):\n",
    "    words = os.path.basename(of).split('_')[0].split('-')\n",
    "    _years = [\"-\".join(words[i:i+span]) for i in range(0, len(words), span)]\n",
    "    smallest = min([year_sizes[s] for s in _years])\n",
    "    min_freq = str(len(_years))\n",
    "    with open(of) as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            if line.strip().split()[0] == min_freq:\n",
    "                count = int(line.strip().split()[1])\n",
    "                break\n",
    "    frequency = 1. * count / smallest\n",
    "    shared_frequencies_by_group.append('{}: {}'.format(', '.join(_years), 100. * frequency))\n",
    "    shared_frequencies_by_group_size[len(_years)].append(frequency)\n",
    "    progress_bar(i + 1, len(cross_year_occurance_files), start_time=start_time)\n",
    "\n",
    "with open('./data/shared_clonotypes/groupwise_shared_clonotype_frequencies.txt', 'w') as f:\n",
    "    f.write('\\n'.join(shared_frequencies_by_group))\n",
    "    \n",
    "with open('./data/shared_clonotypes/groupwise_shared_clonotype_frequencies_by-size.json', 'w') as f:\n",
    "    json.dump(shared_frequencies_by_group_size, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
