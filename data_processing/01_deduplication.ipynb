{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonotype and sequence deduplication\n",
    "\n",
    "Starting with annotated sequence data (in AbStar's `tabular` output format), reduces sequences to clonotypes and collapses dupicate clonotypes.\n",
    "\n",
    "The [`abutils`](https://www.github.com/briney/abutils) Python package is required, and can be installed by running `pip install abutils`\n",
    "\n",
    "*NOTE: this notebook requires the use of the Unix command line tool `sort`. Thus, it requires a Unix-based operating system to run correctly (MacOS and most flavors of Linux should be fine). Running this notebook on Windows 10 may be possible using the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about) but we have not tested this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### years, directories and data fields\n",
    "\n",
    "The input data (annotated sequences in [abstar's](https://github.com/briney/abstar) `tabular` format) is too large to be stored in a Github repository. A compressed archive of the data can be downloaded [**here**](https://burtonlab.s3.amazonaws.com/Collin/Recalled_Leukopaks/techrep_merged/techrep-merged_minimal_no-header.tar.gz). The data file is fairly large (about 400GB uncompressed), so make sure you have enough space before downloading. Decompressing the archive from within the `data` directory (located in the same parent directory as this notebook) will allow the code in this notebook to run without modification. If you would prefer to store the input data somewhere else, be sure to modify the `raw_input_dir` path below.\n",
    "\n",
    "The data fields defined below correspond to the prosition in abstar's `tabular` format. If for some reason you have a differently formatted annotation file, change the field positions to suit your annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years\n",
    "with open('./data/years.txt') as f:\n",
    "    years = sorted(f.read().split())\n",
    "\n",
    "# directories\n",
    "raw_input_dir = './data/techrep_merged/'\n",
    "raw_clonotype_dir = './data/techrep-merged_vj-aa/'\n",
    "make_dir(raw_clonotype_dir)\n",
    "dedup_clonotype_dir = './data/dedup_techrep-merged_vj-aa/'\n",
    "make_dir(dedup_clonotype_dir)\n",
    "dedup_sequence_dir = './data/dedup_techrep-merged_nt-seq/'\n",
    "make_dir(dedup_sequence_dir)\n",
    "logfile = './data/dedup.log'\n",
    "\n",
    "# data fields\n",
    "chain_field=2\n",
    "prod_field = 3\n",
    "v_field = 5\n",
    "j_field = 9\n",
    "cdr3aa_field = 20\n",
    "vdjnt_field = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (biological replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_bioreps(files, raw_clonotype_dir, unique_clonotype_dir,\n",
    "                  raw_sequence_dir, unique_sequence_dir, log_file=None):\n",
    "    # set up output directories\n",
    "    make_dir(raw_clonotype_dir)\n",
    "    make_dir(unique_clonotype_dir)\n",
    "    make_dir(raw_sequence_dir)\n",
    "    make_dir(unique_sequence_dir)\n",
    "    \n",
    "    # process minimal output files\n",
    "    for _f in files:\n",
    "        print(os.path.basename(_f))\n",
    "        clonotype_output_data = []\n",
    "        sequence_output_data = []\n",
    "        raw_clonotype_file = os.path.join(raw_clonotype_dir, os.path.basename(_f))\n",
    "        unique_clonotype_file = os.path.join(unique_clonotype_dir, os.path.basename(_f))\n",
    "        raw_sequence_file = os.path.join(raw_sequence_dir, os.path.basename(_f))\n",
    "        unique_sequence_file = os.path.join(unique_sequence_dir, os.path.basename(_f))\n",
    "        \n",
    "        # collect clonotype/sequence information\n",
    "        with open(_f) as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split(',')\n",
    "                if data[prod_field] != 'yes' or data[chain_field] != 'heavy':\n",
    "                    continue\n",
    "                v_gene = data[v_field]\n",
    "                j_gene = data[j_field]\n",
    "                cdr3_aa = data[cdr3aa_field]\n",
    "                vdj_nt = data[vdjnt_field]\n",
    "                clonotype_output_data.append(' '.join([v_gene, j_gene, cdr3_aa]))\n",
    "                sequence_output_data.append(' '.join([v_gene, j_gene, vdj_nt]))\n",
    "        \n",
    "        # write raw clonotype info to file\n",
    "        raw_clonotype_string = '\\n'.join(clonotype_output_data)\n",
    "        with open(raw_clonotype_file, 'w') as rf:\n",
    "            rf.write(raw_clonotype_string)\n",
    "        raw_clonotype_count = len(clonotype_output_data)\n",
    "        print('raw clonotypes:', raw_clonotype_count)\n",
    "        # collapse duplicate clonotypes (without counts)\n",
    "        uniq_cmd = 'sort -u -o {} -'.format(unique_clonotype_file)\n",
    "        p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, encoding='utf8')\n",
    "        stdout, stderr = p.communicate(input=raw_clonotype_string)\n",
    "        # count the number of unique clonotypes\n",
    "        wc_cmd = 'wc -l {}'.format(unique_clonotype_file)\n",
    "        q = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "        _count, _ = q.communicate()\n",
    "        unique_clonotype_count = int(_count.split()[0])\n",
    "        print('unique clonotypes:', unique_clonotype_count)\n",
    "        if log_file is not None:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write('CLONOTYPES: {} {}\\n'.format(raw_clonotype_count, unique_clonotype_count))\n",
    "                \n",
    "        # write raw sequence info to file\n",
    "        raw_sequence_string = '\\n'.join(sequence_output_data)\n",
    "        with open(raw_sequence_file, 'w') as rf:\n",
    "            rf.write(raw_sequence_string)\n",
    "        raw_sequence_count = len(sequence_output_data)\n",
    "        print('raw sequences:', raw_sequence_count)\n",
    "        # collapse duplicate sequences (without counts)\n",
    "        uniq_cmd = 'sort -u -o {} -'.format(unique_sequence_file)\n",
    "        p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, encoding='utf8')\n",
    "        stdout, stderr = p.communicate(input=raw_sequence_string)\n",
    "        # count the number of unique sequences\n",
    "        wc_cmd = 'wc -l {}'.format(unique_sequence_file)\n",
    "        q = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "        _count, _ = q.communicate()\n",
    "        unique_sequence_count = int(_count.split()[0])\n",
    "        print('unique sequences:', unique_sequence_count)\n",
    "        if log_file is not None:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write('SEQUENCES: {} {}\\n'.format(raw_sequence_count, unique_sequence_count))\n",
    "        \n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "327059-2016\n",
      "---------\n",
      "\n",
      "327059-2016_biorep1\n",
      "raw clonotypes: 10398662\n",
      "unique clonotypes: 4315442\n",
      "raw sequences: 10398662\n",
      "unique sequences: 6838434\n",
      "\n",
      "327059-2016_biorep2\n",
      "raw clonotypes: 6672566\n",
      "unique clonotypes: 2065391\n",
      "raw sequences: 6672566\n",
      "unique sequences: 4130101\n",
      "\n",
      "327059-2016_biorep3\n",
      "raw clonotypes: 4266975\n",
      "unique clonotypes: 1237575\n",
      "raw sequences: 4266975\n",
      "unique sequences: 2619620\n",
      "\n",
      "327059-2016_biorep4\n",
      "raw clonotypes: 2536805\n",
      "unique clonotypes: 641605\n",
      "raw sequences: 2536805\n",
      "unique sequences: 1533250\n",
      "\n",
      "327059-2016_biorep5\n",
      "raw clonotypes: 4164872\n",
      "unique clonotypes: 1176969\n",
      "raw sequences: 4164872\n",
      "unique sequences: 2567687\n",
      "\n",
      "327059-2016_biorep6\n",
      "raw clonotypes: 3279502\n",
      "unique clonotypes: 909372\n",
      "raw sequences: 3279502\n",
      "unique sequences: 2018997\n",
      "\n",
      "\n",
      "---------\n",
      "327059-2020\n",
      "---------\n",
      "\n",
      "327059-2020_biorep1\n",
      "raw clonotypes: 2270314\n",
      "unique clonotypes: 813040\n",
      "raw sequences: 2270314\n",
      "unique sequences: 1602663\n",
      "\n",
      "327059-2020_biorep2\n",
      "raw clonotypes: 4826120\n",
      "unique clonotypes: 1964478\n",
      "raw sequences: 4826120\n",
      "unique sequences: 3382748\n",
      "\n",
      "327059-2020_biorep3\n",
      "raw clonotypes: 620892\n",
      "unique clonotypes: 291526\n",
      "raw sequences: 620892\n",
      "unique sequences: 472240\n",
      "\n",
      "327059-2020_biorep4\n",
      "raw clonotypes: 1347608\n",
      "unique clonotypes: 534364\n",
      "raw sequences: 1347608\n",
      "unique sequences: 973290\n",
      "\n",
      "327059-2020_biorep5\n",
      "raw clonotypes: 2326862\n",
      "unique clonotypes: 850063\n",
      "raw sequences: 2326862\n",
      "unique sequences: 1643289\n",
      "\n",
      "327059-2020_biorep6\n",
      "raw clonotypes: 1324304\n",
      "unique clonotypes: 530260\n",
      "raw sequences: 1324304\n",
      "unique sequences: 966885\n",
      "\n",
      "\n",
      "---------\n",
      "D103-2016\n",
      "---------\n",
      "\n",
      "D103-2016_biorep1\n",
      "raw clonotypes: 3300309\n",
      "unique clonotypes: 1204569\n",
      "raw sequences: 3300309\n",
      "unique sequences: 2080217\n",
      "\n",
      "D103-2016_biorep2\n",
      "raw clonotypes: 1085936\n",
      "unique clonotypes: 238789\n",
      "raw sequences: 1085936\n",
      "unique sequences: 587884\n",
      "\n",
      "D103-2016_biorep3\n",
      "raw clonotypes: 2384694\n",
      "unique clonotypes: 597149\n",
      "raw sequences: 2384694\n",
      "unique sequences: 1338927\n",
      "\n",
      "D103-2016_biorep4\n",
      "raw clonotypes: 2441457\n",
      "unique clonotypes: 581096\n",
      "raw sequences: 2441457\n",
      "unique sequences: 1349366\n",
      "\n",
      "D103-2016_biorep5\n",
      "raw clonotypes: 1170305\n",
      "unique clonotypes: 267265\n",
      "raw sequences: 1170305\n",
      "unique sequences: 639348\n",
      "\n",
      "D103-2016_biorep6\n",
      "raw clonotypes: 1479183\n",
      "unique clonotypes: 363643\n",
      "raw sequences: 1479183\n",
      "unique sequences: 830236\n",
      "\n",
      "\n",
      "---------\n",
      "D103-2021\n",
      "---------\n",
      "\n",
      "D103-2021_biorep1\n",
      "raw clonotypes: 158158\n",
      "unique clonotypes: 116615\n",
      "raw sequences: 158158\n",
      "unique sequences: 137475\n",
      "\n",
      "D103-2021_biorep2\n",
      "raw clonotypes: 246562\n",
      "unique clonotypes: 174350\n",
      "raw sequences: 246562\n",
      "unique sequences: 209839\n",
      "\n",
      "D103-2021_biorep3\n",
      "raw clonotypes: 302899\n",
      "unique clonotypes: 165501\n",
      "raw sequences: 302899\n",
      "unique sequences: 224968\n",
      "\n",
      "D103-2021_biorep4\n",
      "raw clonotypes: 664687\n",
      "unique clonotypes: 411270\n",
      "raw sequences: 664687\n",
      "unique sequences: 519171\n",
      "\n",
      "D103-2021_biorep5\n",
      "raw clonotypes: 1506424\n",
      "unique clonotypes: 699107\n",
      "raw sequences: 1506424\n",
      "unique sequences: 1022234\n",
      "\n",
      "D103-2021_biorep6\n",
      "raw clonotypes: 2121741\n",
      "unique clonotypes: 658059\n",
      "raw sequences: 2121741\n",
      "unique sequences: 1221216\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear the logfile\n",
    "with open(logfile, 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "# iteratively process each year\n",
    "for year in years:\n",
    "    print('---------')\n",
    "    print(year)\n",
    "    print('---------')\n",
    "    print()\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write('#' + year + '\\n')\n",
    "    files = list_files('./data/techrep_merged/{}'.format(year))\n",
    "    raw_clonotype_dir = './data/techrep-merged_vj-aa/{}'.format(year)\n",
    "    unique_clonotype_dir = './data/dedup_techrep-merged_vj-aa/{}'.format(year)\n",
    "    raw_sequence_dir = './data/techrep-merged_vdj-nt/{}'.format(year)\n",
    "    unique_sequence_dir = './data/dedup_techrep-merged_vdj-nt/{}'.format(year)\n",
    "    dedup_bioreps(files, raw_clonotype_dir, unique_clonotype_dir,\n",
    "                  raw_sequence_dir, unique_sequence_dir, log_file=logfile)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (year pools)\n",
    "\n",
    "In the previous blocks of code, we created a unique clonotype file for each biological replicate for each year. Here, we'd like to create a single file for each year containing only unique clonotypes (regardless of which biological replicate they came from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_clonotype_year_pool_dir = './data/dedup_year_clonotype_pools/'\n",
    "dedup_sequence_year_pool_dir = './data/dedup_year_sequence_pools/'\n",
    "make_dir(dedup_clonotype_year_pool_dir)\n",
    "make_dir(dedup_sequence_year_pool_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to create a unique clonotype file for each year that also contains the number of times we saw each clonotype (using the deduplicated biological replicates, so the clonotype count essentially tallies the number of biological replicates in which we observed each clonotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327059-2016\n"
     ]
    }
   ],
   "source": [
    "for year in years[:1]:\n",
    "    print(year)\n",
    "    \n",
    "    # clonotypes\n",
    "    input_clonotype_files = list_files(os.path.join(dedup_clonotype_dir, year))\n",
    "    ofile = os.path.join(dedup_clonotype_year_pool_dir, '{}_dedup_pool_vj-aa_with-counts.txt'.format(year))\n",
    "    uniq_cmd = 'cat {} | sort | uniq -c > {}'.format(' '.join(input_clonotype_files), ofile)\n",
    "    c = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = c.communicate()\n",
    "    \n",
    "    # sequences\n",
    "    input_sequence_files = list_files(os.path.join(dedup_sequence_dir, year))\n",
    "    ofile = os.path.join(dedup_sequence_year_pool_dir, '{}_dedup_pool_vdj-nt_with-counts.txt'.format(year))\n",
    "    uniq_cmd = 'cat {} | sort | uniq -c > {}'.format(' '.join(input_sequence_files), ofile)\n",
    "    s = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = s.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same process, but without counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327059-2016\n",
      "327059-2020\n",
      "D103-2016\n",
      "D103-2021\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(year)\n",
    "        \n",
    "    # clonotypes\n",
    "    input_clonotype_files = list_files(os.path.join(dedup_clonotype_dir, year))\n",
    "    ofile = os.path.join(dedup_clonotype_year_pool_dir, '{}_dedup_pool_vj-aa.txt'.format(year))\n",
    "    uniq_cmd = 'cat {} | sort | uniq > {}'.format(' '.join(input_clonotype_files), ofile)\n",
    "    c = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = c.communicate()\n",
    "    \n",
    "    # sequences\n",
    "    input_sequence_files = list_files(os.path.join(dedup_sequence_dir, year))\n",
    "    ofile = os.path.join(dedup_sequence_year_pool_dir, '{}_dedup_pool_vdj-nt.txt'.format(year))\n",
    "    uniq_cmd = 'cat {} | sort | uniq > {}'.format(' '.join(input_sequence_files), ofile)\n",
    "    s = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = s.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (cross-year pools)\n",
    "\n",
    "Finally, we'd like to create unique clonotype files (with counts) for every groupwise combination of our 3 years. Each group can contain two or more years, meaning the total number of possible groupwise combinations is quite large. We'll use the `multiprocessing` package to parallelize the process which should speed things up substantially, although even with parallelization, this will take some time.\n",
    "\n",
    "***NOTE:*** *The output from the following code blocks will be quite large (deduplicated clonotype files are >2TB in total, deduplicated sequence files are >20TB in total). Make sure you have sufficient storage and that the output paths below (`dedup_cross_year_clonotype_pool_dir` and `dedup_cross_year_sequence_pool_dir` are correct before starting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "dedup_cross_year_clonotype_pool_dir = './data/dedup_cross-year_clonotype_pools/'\n",
    "dedup_cross_year_sequence_pool_dir = './data/dedup_cross-year_sequence_pools/'\n",
    "make_dir(dedup_cross_year_clonotype_pool_dir)\n",
    "make_dir(dedup_cross_year_sequence_pool_dir)\n",
    "\n",
    "# deduplicated year pool files\n",
    "dedup_clonotype_year_files = [f for f in list_files(dedup_clonotype_year_pool_dir) if '_dedup_pool_vj-aa.txt' in f]\n",
    "dedup_sequence_year_files = [f for f in list_files(dedup_sequence_year_pool_dir) if '_dedup_pool_vdj-nt.txt' in f]\n",
    "\n",
    "# every possible groupwise combination of years (2 or more years per group)\n",
    "year_combinations_by_size = {}\n",
    "for size in range(2, 5):\n",
    "    year_combinations_by_size[size] = [sorted(c) for c in itertools.combinations(years, size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_cross_year_pool(years, dedup_year_files, output_dir, file_end, temp_dir='/tmp/'):\n",
    "    files = sorted(list(set([f for f in dedup_year_files if os.path.basename(f).split('_')[0] in years])))\n",
    "    output_file = os.path.join(output_dir, '{}{}'.format('-'.join(years), file_end))\n",
    "    uniq_cmd = 'cat {} | sort -T {} | uniq -c > {}'.format(' '.join(files), temp_dir, output_file)\n",
    "    p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True, encoding='utf8')\n",
    "    stdout, stderr = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clonotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-year pools:\n",
      "(6/6) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "(6/6) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "\n",
    "for size in sorted(year_combinations_by_size.keys())[:1]:\n",
    "    year_combinations = year_combinations_by_size[size]\n",
    "    async_results = []\n",
    "    print('{}-year pools:'.format(size))\n",
    "    progress_bar(0, len(year_combinations))\n",
    "    for sub_comb in year_combinations:\n",
    "        files = sorted(list(set([f for f in dedup_clonotype_year_files if os.path.basename(f).split('_')[0] in sub_comb])))\n",
    "        async_results.append(p.apply_async(dedup_cross_year_pool,\n",
    "                                           args=(sub_comb, files, dedup_cross_year_clonotype_pool_dir, '_dedup_pool_vj-aa_with-counts.txt')))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    print('\\n')\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences\n",
    "\n",
    "Just one more warning that the following code block will produce a very large amount of data (>20TB) and will take many hours to run even on a fairly robust server (an `m4.16xlarge` AWS EC2 instance, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-year pools:\n",
      "(6/6) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "(6/6) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "\n",
    "for size in sorted(year_combinations_by_size.keys())[:1]:\n",
    "    year_combinations = year_combinations_by_size[size]\n",
    "    async_results = []\n",
    "    print('{}-year pools:'.format(size))\n",
    "    progress_bar(0, len(year_combinations))\n",
    "    for sub_comb in year_combinations:\n",
    "        files = sorted(list(set([f for f in dedup_sequence_year_files if os.path.basename(f).split('_')[0] in sub_comb])))\n",
    "        async_results.append(p.apply_async(dedup_cross_year_pool,\n",
    "                                           args=(sub_comb, files, dedup_cross_year_sequence_pool_dir, '_dedup_pool_vdj-nt_with-counts.txt')))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    print('\\n')\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
